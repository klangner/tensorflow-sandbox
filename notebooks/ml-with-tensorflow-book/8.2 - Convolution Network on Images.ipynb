{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    data = pickle.load(fo, encoding='latin1')\n",
    "    fo.close\n",
    "    return data\n",
    "\n",
    "\n",
    "def clean(data):\n",
    "    # We need to fix shape\n",
    "    imgs = data.reshape(data.shape[0], 3, 32, 32).astype(np.float32)\n",
    "    # Naive grayscala conversion\n",
    "    grayscala_imgs = imgs.mean(1)\n",
    "    # For faster processing we only need 24x24 images\n",
    "    cropped_imgs = grayscala_imgs[:, 4:28, 4:28]\n",
    "    img_data = cropped_imgs.reshape(data.shape[0], -1)\n",
    "    img_size = np.shape(img_data)[1]\n",
    "    means = np.mean(img_data, axis=1)\n",
    "    meansT = means.reshape(len(means), 1)\n",
    "    stds = np.std(img_data, axis=1)\n",
    "    stdsT = stds.reshape(len(stds), 1)\n",
    "    adj_stds = np.maximum(stdsT, 1.0 / np.sqrt(img_size))\n",
    "    normalized = (img_data - meansT) / adj_stds\n",
    "    return normalized\n",
    "    \n",
    "    \n",
    "def load_images(path):\n",
    "    names = unpickle(path + 'batches.meta')['label_names']\n",
    "    data, labels = [], []\n",
    "    for i in range(1, 6):\n",
    "        filename = path + 'data_batch_' + str(i)\n",
    "        batch_data = unpickle(filename)\n",
    "        if len(data) > 0:\n",
    "            data = np.vstack([data, batch_data['data']])\n",
    "            labels = np.hstack([labels, batch_data['labels']])\n",
    "        else:\n",
    "            data = batch_data['data']\n",
    "            labels = batch_data['labels']\n",
    "    data = clean(data)\n",
    "    return names, data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names, data, labels = load_images('../datasets/cifar-10-batches-py/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 24*24])\n",
    "y = tf.placeholder(tf.float32, [None, len(names)])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([5, 5, 1, 64]))\n",
    "b1 = tf.Variable(tf.random_normal([64]))\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([5, 5, 64, 64]))\n",
    "b2 = tf.Variable(tf.random_normal([64]))\n",
    "\n",
    "# Fuly connected layer\n",
    "W3 = tf.Variable(tf.random_normal([6*6*64, 1024]))\n",
    "b3 = tf.Variable(tf.random_normal([1024]))\n",
    "\n",
    "W_out = tf.Variable(tf.random_normal([1024, len(names)]))\n",
    "b_out = tf.Variable(tf.random_normal([len(names)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions for creating ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(x, W, b):\n",
    "    conv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv_with_b = tf.nn.bias_add(conv, b)\n",
    "    return tf.nn.relu(conv_with_b)\n",
    "\n",
    "\n",
    "def maxpool_layer(conv, k=2):\n",
    "    return tf.nn.max_pool(conv, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def model():\n",
    "    x_reshaped = tf.reshape(x, shape=[-1, 24, 24, 1])\n",
    "\n",
    "    conv_out1 = conv_layer(x_reshaped, W1, b1)\n",
    "    maxpool_out1 = maxpool_layer(conv_out1)\n",
    "    norm1 = tf.nn.lrn(maxpool_out1, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)\n",
    "\n",
    "    conv_out2 = conv_layer(norm1, W2, b2)\n",
    "    maxpool_out2 = maxpool_layer(conv_out2)\n",
    "    norm2 = tf.nn.lrn(maxpool_out2, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)    \n",
    "    \n",
    "    maxpool_reshaped = tf.reshape(norm2, [-1, W3.get_shape().as_list()[0]])\n",
    "    local = tf.add(tf.matmul(maxpool_reshaped, W3), b3)\n",
    "    local_out = tf.nn.relu(local)\n",
    "    \n",
    "    return tf.add(tf.matmul(local_out, W_out), b_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_op = model()\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model_op, labels=y))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(model_op, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 50000\n",
      "batch_size 250\n",
      "Epoch 0\n",
      "0 0.088\n",
      "10000 0.228\n",
      "20000 0.216\n",
      "30000 0.228\n",
      "40000 0.244\n",
      "Epoch 1\n",
      "0 0.256\n",
      "10000 0.32\n",
      "20000 0.296\n",
      "30000 0.268\n",
      "40000 0.244\n",
      "Epoch 2\n",
      "0 0.24\n",
      "10000 0.308\n",
      "20000 0.32\n",
      "30000 0.276\n",
      "40000 0.284\n",
      "Epoch 3\n",
      "0 0.272\n",
      "10000 0.304\n",
      "20000 0.324\n",
      "30000 0.308\n",
      "40000 0.304\n",
      "Epoch 4\n",
      "0 0.304\n",
      "10000 0.3\n",
      "20000 0.308\n",
      "30000 0.312\n",
      "40000 0.32\n",
      "Epoch 5\n",
      "0 0.348\n",
      "10000 0.3\n",
      "20000 0.312\n",
      "30000 0.308\n",
      "40000 0.328\n",
      "Epoch 6\n",
      "0 0.32\n",
      "10000 0.288\n",
      "20000 0.336\n",
      "30000 0.332\n",
      "40000 0.34\n",
      "Epoch 7\n",
      "0 0.352\n",
      "10000 0.284\n",
      "20000 0.328\n",
      "30000 0.324\n",
      "40000 0.328\n",
      "Epoch 8\n",
      "0 0.348\n",
      "10000 0.296\n",
      "20000 0.348\n",
      "30000 0.368\n",
      "40000 0.332\n",
      "Epoch 9\n",
      "0 0.356\n",
      "10000 0.328\n",
      "20000 0.352\n",
      "30000 0.376\n",
      "40000 0.34\n",
      "Done. Accuracy: 0.3720000088214874\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print('Dataset size: {}'.format(np.shape(data)[0]))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    onehot_labels = tf.one_hot(labels, len(names), on_value=1.0, off_value=0.0, axis=-1)\n",
    "    onehot_vals = sess.run(onehot_labels)\n",
    "    batch_size = len(data) // 200\n",
    "    print('batch_size', batch_size)\n",
    "    for j in range(10):\n",
    "        print('Epoch', j)\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            batch_data = data[i:i+batch_size, :]\n",
    "            batch_onehot_vals = onehot_vals[i:i+batch_size, :]\n",
    "            _, accuracy_val = sess.run([train_op, accuracy], feed_dict={x:batch_data, y: batch_onehot_vals})\n",
    "            if i % 10000 == 0:\n",
    "                print(i, accuracy_val)\n",
    "    print('Done. Accuracy: {}'.format(accuracy_val))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
