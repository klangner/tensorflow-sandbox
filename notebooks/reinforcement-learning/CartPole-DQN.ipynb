{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve CartPole using DQN\n",
    "\n",
    "https://gym.openai.com/envs/CartPole-v0/\n",
    "\n",
    "Some useful resources:\n",
    "  * https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from keras import models, layers, optimizers\n",
    "from replay_buffer import ReplayBuffer\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Observation shape: (4,)\n",
      "Number of actions: 2\n",
      "Example state: [ 0.02773734 -0.02999205  0.02214671 -0.00825152]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbd2a5539b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAEppJREFUeJzt3X+s3Xd93/Hnq3FIGLA6ITeR6x9z\nKO5KOg0nvQtGmaY0oW2SrXMqlSrpVCIU6TIpSKCirUknDZAWqZVWsqF1EW6TYipGyAI0XpSWZiao\n4g8SbDDGiUlzAYNv7cXOSAIMLZvDe3/czyVnzvG9x/fc6+v74fmQjs73+zmf7/e8P+Twut/7ud+P\nT6oKSVJ/fmqlC5AkLQ8DXpI6ZcBLUqcMeEnqlAEvSZ0y4CWpU8sW8EmuS/JUkukkty/X+0iShsty\n3Aef5Bzgb4BfBmaALwE3V9WTS/5mkqShlusK/kpguqq+WVX/B7gP2L5M7yVJGmLNMp13PXB4YH8G\neMupOl900UW1efPmZSpFklafQ4cO8eyzz2accyxXwA8r6v+bC0oyBUwBbNq0iT179ixTKZK0+kxO\nTo59juWaopkBNg7sbwCODHaoqh1VNVlVkxMTE8tUhiT95FqugP8SsCXJpUleBdwE7Fqm95IkDbEs\nUzRVdSLJu4HPAucA91bVE8vxXpKk4ZZrDp6qehh4eLnOL0manytZJalTBrwkdcqAl6ROGfCS1CkD\nXpI6ZcBLUqcMeEnqlAEvSZ0y4CWpUwa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6pQBL0mdMuAl\nqVMGvCR1aqyv7EtyCPg+8BJwoqomk1wIfBLYDBwCfrOqnhuvTEnS6VqKK/hfqqqtVTXZ9m8HdlfV\nFmB325cknWHLMUWzHdjZtncCNy7De0iSFjBuwBfwV0n2JplqbZdU1VGA9nzxmO8hSVqEsebggauq\n6kiSi4FHknx91APbD4QpgE2bNo1ZhiTpZGNdwVfVkfZ8DPgMcCXwTJJ1AO352CmO3VFVk1U1OTEx\nMU4ZkqQhFh3wSV6T5HVz28CvAAeAXcAtrdstwIPjFilJOn3jTNFcAnwmydx5/ktV/WWSLwH3J7kV\n+A7w9vHLlCSdrkUHfFV9E3jzkPb/CVw7TlGSpPG5klWSOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1\nyoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqcM\neEnq1IIBn+TeJMeSHBhouzDJI0mebs8XtPYk+XCS6ST7k1yxnMVLkk5tlCv4jwLXndR2O7C7qrYA\nu9s+wPXAlvaYAu5emjIlSadrwYCvqr8GvntS83ZgZ9veCdw40P6xmvVFYG2SdUtVrCRpdIudg7+k\nqo4CtOeLW/t64PBAv5nW9gpJppLsSbLn+PHjiyxDknQqS/1H1gxpq2Edq2pHVU1W1eTExMQSlyFJ\nWmzAPzM39dKej7X2GWDjQL8NwJHFlydJWqzFBvwu4Ja2fQvw4ED7O9rdNNuAF+amciRJZ9aahTok\n+QRwNXBRkhng/cDvA/cnuRX4DvD21v1h4AZgGvgh8M5lqFmSNIIFA76qbj7FS9cO6VvAbeMWJUka\nnytZJalTBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAEvSZ0y4CWpUwa8JHXKgJekThnwktQp\nA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1asGAT3JvkmNJDgy0fSDJ3ybZ1x43DLx2R5LpJE8l\n+dXlKlySNL9RruA/Clw3pP2uqtraHg8DJLkMuAn4hXbMf05yzlIVK0ka3YIBX1V/DXx3xPNtB+6r\nqher6lvANHDlGPVJkhZpnDn4dyfZ36ZwLmht64HDA31mWtsrJJlKsifJnuPHj49RhiRpmMUG/N3A\nzwJbgaPAH7b2DOlbw05QVTuqarKqJicmJhZZhiTpVBYV8FX1TFW9VFU/Av6Yl6dhZoCNA103AEfG\nK1GStBiLCvgk6wZ2fx2Yu8NmF3BTkvOSXApsAR4fr0RJ0mKsWahDkk8AVwMXJZkB3g9cnWQrs9Mv\nh4B3AVTVE0nuB54ETgC3VdVLy1O6JGk+CwZ8Vd08pPmeefrfCdw5TlGSpPG5klWSOmXAS1KnDHhJ\n6pQBL0mdMuAlqVMGvCR1asHbJKWfNHt3vOsVbb849ZEVqEQaj1fwktQpA16SOmXAS1KnDHhJ6pQB\nL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpxYM+CQbkzya5GCSJ5K8p7VfmOSRJE+35wta\ne5J8OMl0kv1JrljuQUiSXmmUK/gTwPuq6k3ANuC2JJcBtwO7q2oLsLvtA1wPbGmPKeDuJa9akrSg\nBQO+qo5W1Zfb9veBg8B6YDuws3XbCdzYtrcDH6tZXwTWJlm35JVLkuZ1WnPwSTYDlwOPAZdU1VGY\n/SEAXNy6rQcODxw209pOPtdUkj1J9hw/fvz0K5ckzWvkgE/yWuBTwHur6nvzdR3SVq9oqNpRVZNV\nNTkxMTFqGdKyGvZvwUur1UgBn+RcZsP941X16db8zNzUS3s+1tpngI0Dh28AjixNuZKkUY1yF02A\ne4CDVfWhgZd2Abe07VuABwfa39HuptkGvDA3lSNJOnNG+cq+q4DfBr6WZF9r+z3g94H7k9wKfAd4\ne3vtYeAGYBr4IfDOJa1YkjSSBQO+qr7A8Hl1gGuH9C/gtjHrkiSNyZWsktQpA16SOmXAS1KnDHhJ\n6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6RO\nGfCS1CkDXpI6NcqXbm9M8miSg0meSPKe1v6BJH+bZF973DBwzB1JppM8leRXl3MAkqThRvnS7RPA\n+6rqy0leB+xN8kh77a6q+veDnZNcBtwE/ALwM8B/T/JzVfXSUhYunSm/OPWRlS5BWpQFr+Cr6mhV\nfbltfx84CKyf55DtwH1V9WJVfQuYBq5cimIlSaM7rTn4JJuBy4HHWtO7k+xPcm+SC1rbeuDwwGEz\nzP8DQZK0DEYO+CSvBT4FvLeqvgfcDfwssBU4CvzhXNchh9eQ800l2ZNkz/Hjx0+7cEnS/EYK+CTn\nMhvuH6+qTwNU1TNV9VJV/Qj4Y16ehpkBNg4cvgE4cvI5q2pHVU1W1eTExMQ4Y5AkDTHKXTQB7gEO\nVtWHBtrXDXT7deBA294F3JTkvCSXAluAx5euZEnSKEa5i+Yq4LeBryXZ19p+D7g5yVZmp18OAe8C\nqKonktwPPMnsHTi3eQeNJJ15CwZ8VX2B4fPqD89zzJ3AnWPUJUkakytZJalTBrwkdcqAl6ROGfCS\n1CkDXpI6ZcBLUqcMeEnqlAEvSZ0y4CWpUwa8JHXKgJekThnwktQpA16SOmXAq3tJRn4sx/HSSjHg\nJalTo3zhh/QT5b8dmfrx9q/9zI4VrEQaj1fw0oDBcB+2L60mBrzUvP/9e1a6BGlJjfKl2+cneTzJ\nV5M8keSDrf3SJI8leTrJJ5O8qrWf1/an2+ubl3cI0tJwOka9GeUK/kXgmqp6M7AVuC7JNuAPgLuq\nagvwHHBr638r8FxVvRG4q/WTVoWTQ97Q12o2ypduF/CDtntuexRwDfBbrX0n8AHgbmB72wZ4APhP\nSdLOI521Jt81F+Yvh/oHV6YUaUmMdBdNknOAvcAbgT8CvgE8X1UnWpcZYH3bXg8cBqiqE0leAF4P\nPHuq8+/du9d7iNUFP8c6m4wU8FX1ErA1yVrgM8CbhnVrz8M+4a+4ek8yBUwBbNq0iW9/+9sjFSyd\nrjMZuv6iqqUyOTk59jlO6y6aqnoe+DywDVibZO4HxAbgSNueATYCtNd/GvjukHPtqKrJqpqcmJhY\nXPWSpFMa5S6aiXblTpJXA28DDgKPAr/Rut0CPNi2d7V92uufc/5dks68UaZo1gE72zz8TwH3V9VD\nSZ4E7kvy74CvAPe0/vcAf5Zkmtkr95uWoW5J0gJGuYtmP3D5kPZvAlcOaf/fwNuXpDpJ0qK5klWS\nOmXAS1KnDHhJ6pT/XLC6501c+knlFbwkdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAEvSZ0y4CWp\nUwa8JHXKgJekThnwktQpA16SOmXAS1KnDHhJ6tQoX7p9fpLHk3w1yRNJPtjaP5rkW0n2tcfW1p4k\nH04ynWR/kiuWexCSpFca5d+DfxG4pqp+kORc4AtJ/qK99q+q6oGT+l8PbGmPtwB3t2dJ0hm04BV8\nzfpB2z23Peb7BoXtwMfacV8E1iZZN36pkqTTMdIcfJJzkuwDjgGPVNVj7aU72zTMXUnOa23rgcMD\nh8+0NknSGTRSwFfVS1W1FdgAXJnkHwB3AD8P/CPgQuB3W/cMO8XJDUmmkuxJsuf48eOLKl6SdGqn\ndRdNVT0PfB64rqqOtmmYF4E/Ba5s3WaAjQOHbQCODDnXjqqarKrJiYmJRRUvSTq1Ue6imUiytm2/\nGngb8PW5efUkAW4EDrRDdgHvaHfTbANeqKqjy1K9JOmURrmLZh2wM8k5zP5AuL+qHkryuSQTzE7J\n7AP+Zev/MHADMA38EHjn0pctSVrIggFfVfuBy4e0X3OK/gXcNn5pkqRxuJJVkjplwEtSpwx4SeqU\nAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAEvSZ0y4CWpUwa8JHXKgJekThnw\nktQpA16SOmXAS1KnDHhJ6tTIAZ/knCRfSfJQ2780yWNJnk7yySSvau3ntf3p9vrm5SldkjSf07mC\nfw9wcGD/D4C7qmoL8Bxwa2u/FXiuqt4I3NX6SZLOsJECPskG4J8Cf9L2A1wDPNC67ARubNvb2z7t\n9Wtbf0nSGbRmxH7/AfjXwOva/uuB56vqRNufAda37fXAYYCqOpHkhdb/2cETJpkCptrui0kOLGoE\nZ7+LOGnsneh1XNDv2BzX6vL3kkxV1Y7FnmDBgE/yz4BjVbU3ydVzzUO61givvdwwW/SO9h57qmpy\npIpXmV7H1uu4oN+xOa7VJ8keWk4uxihX8FcB/zzJDcD5wN9l9op+bZI17Sp+A3Ck9Z8BNgIzSdYA\nPw18d7EFSpIWZ8E5+Kq6o6o2VNVm4Cbgc1X1L4BHgd9o3W4BHmzbu9o+7fXPVdUrruAlSctrnPvg\nfxf4nSTTzM6x39Pa7wFe39p/B7h9hHMt+leQVaDXsfU6Luh3bI5r9RlrbPHiWpL65EpWSerUigd8\nkuuSPNVWvo4ynXNWSXJvkmODt3kmuTDJI22V7yNJLmjtSfLhNtb9Sa5Yucrnl2RjkkeTHEzyRJL3\ntPZVPbYk5yd5PMlX27g+2Nq7WJnd64rzJIeSfC3JvnZnyar/LAIkWZvkgSRfb/9fe+tSjmtFAz7J\nOcAfAdcDlwE3J7lsJWtahI8C153Udjuwu63y3c3Lf4e4HtjSHlPA3WeoxsU4Abyvqt4EbANua/9t\nVvvYXgSuqao3A1uB65Jso5+V2T2vOP+lqto6cEvkav8sAvxH4C+r6ueBNzP7327pxlVVK/YA3gp8\ndmD/DuCOlaxpkePYDBwY2H8KWNe21wFPte2PADcP63e2P5i9S+qXexob8HeALwNvYXahzJrW/uPP\nJfBZ4K1te03rl5Wu/RTj2dAC4RrgIWbXpKz6cbUaDwEXndS2qj+LzN5y/q2T/3dfynGt9BTNj1e9\nNoMrYlezS6rqKEB7vri1r8rxtl/fLwceo4OxtWmMfcAx4BHgG4y4MhuYW5l9Nppbcf6jtj/yinPO\n7nHB7GLJv0qyt62Ch9X/WXwDcBz40zat9idJXsMSjmulA36kVa8dWXXjTfJa4FPAe6vqe/N1HdJ2\nVo6tql6qqq3MXvFeCbxpWLf2vCrGlYEV54PNQ7quqnENuKqqrmB2muK2JP9knr6rZWxrgCuAu6vq\ncuB/Mf9t5ac9rpUO+LlVr3MGV8SuZs8kWQfQno+19lU13iTnMhvuH6+qT7fmLsYGUFXPA59n9m8M\na9vKaxi+MpuzfGX23IrzQ8B9zE7T/HjFeeuzGscFQFUdac/HgM8w+4N5tX8WZ4CZqnqs7T/AbOAv\n2bhWOuC/BGxpf+l/FbMrZXetcE1LYXA178mrfN/R/hq+DXhh7lexs02SMLto7WBVfWjgpVU9tiQT\nSda27VcDb2P2D1uremV2dbziPMlrkrxubhv4FeAAq/yzWFX/Azic5O+3pmuBJ1nKcZ0Ff2i4Afgb\nZudB/81K17OI+j8BHAX+L7M/YW9ldi5zN/B0e76w9Q2zdw19A/gaMLnS9c8zrn/M7K9/+4F97XHD\nah8b8A+Br7RxHQD+bWt/A/A4MA38V+C81n5+259ur79hpccwwhivBh7qZVxtDF9tjyfmcmK1fxZb\nrVuBPe3z+OfABUs5LleySlKnVnqKRpK0TAx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6\n9f8AKJ2EmNQD02gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbd3bc75780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#spawn game instance for tests\n",
    "env = gym.make(\"CartPole-v0\") #create raw env\n",
    "\n",
    "observation_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"Observation shape: {}\".format(observation_shape))\n",
    "print(\"Number of actions: {}\".format(n_actions))\n",
    "\n",
    "print(\"Example state: {}\".format(env.reset()))\n",
    "plt.imshow(env.render('rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define QNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0050894 , 0.00561004], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class QNetwork:\n",
    "    \n",
    "    def __init__(self, input_shape, n_actions, alpha=0.0003):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.model = self._build_network()\n",
    "\n",
    "    def _build_network(self):\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.InputLayer(self.input_shape))\n",
    "        model.add(layers.Dense(64, activation='relu'))\n",
    "        model.add(layers.Dense(self.output_shape, activation='linear'))\n",
    "        opt = optimizers.RMSprop(lr=self.alpha)\n",
    "        model.compile(loss='mse', optimizer=opt)\n",
    "        return model\n",
    "    \n",
    "    def predict(self, state):\n",
    "        \"\"\"Make prediction for single state and return q values for all actions\"\"\"\n",
    "        s = np.expand_dims(state, axis=0)\n",
    "        return self.model.predict(s)[0]\n",
    "    \n",
    "    def predict_batch(self, states):\n",
    "        \"\"\"Make prediction for list of states\"\"\"\n",
    "        return self.model.predict(states)\n",
    "    \n",
    "    def train(self, x, y):\n",
    "        self.model.fit(x, y, batch_size=64)\n",
    "\n",
    "        \n",
    "network = QNetwork(observation_shape, n_actions)\n",
    "network.predict(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build traing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "def build_training_set(qvalues, qvalues_next, actions, rewards, dones, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Create training set for QNetwork.\n",
    "    Params:\n",
    "      qvalues           - Q values for the starting state\n",
    "      qvalues_next      - Q values for the state the next state\n",
    "      actions           - Actions taken\n",
    "      rewards           - Rewards received after taking action \n",
    "      dones             - Did this action end the episode?\n",
    "      \n",
    "    Returns:\n",
    "      Expected qvalues\n",
    "    \"\"\"\n",
    "    y = qvalues.copy()\n",
    "    next_rewards = np.where(dones, np.zeros(rewards.shape), np.max(qvalues_next, axis=1))\n",
    "    y[np.arange(y.shape[0]), actions] = rewards + gamma * next_rewards\n",
    "    return y\n",
    "\n",
    "\n",
    "# Some tests\n",
    "qvalues = np.zeros((5, n_actions))\n",
    "qvalues2 = np.ones((5, n_actions))\n",
    "actions = np.array([0, 1, 0, 1, 0])\n",
    "rewards = np.array([1, 2, 3, 4, 5])\n",
    "dones = np.array([False, False, False, False, True])\n",
    "expected_y = np.array([[2, 0], [0, 3], [4, 0], [0, 5], [5, 0]])\n",
    "y = build_training_set(qvalues, qvalues2, actions, rewards, dones, 1.0)\n",
    "assert np.array_equal(y, expected_y), 'Wrong expected qvalue calculated'\n",
    "print('Ok!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.5171\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, network):\n",
    "        self.memory_capacity = 100000\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_max = 0.01\n",
    "        self.epsilon_lambda = 0.001\n",
    "        self.batch_size = 64\n",
    "        self.epsilon = self.epsilon_max        \n",
    "        self.model = network\n",
    "        self.replays = ReplayBuffer(self.memory_capacity)\n",
    "        self.step = 0\n",
    "        \n",
    "    def policy(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.randint(0, network.n_actions)\n",
    "        else:\n",
    "            qvalues = self.model.predict(state)\n",
    "            action = np.argmax(qvalues)\n",
    "            self.step += 1\n",
    "        self.epsilon = self.epsilon_min + (self.epsilon_max-self.epsilon_min) * np.power(np.e, self.epsilon_lambda*self.step)\n",
    "        return action\n",
    "    \n",
    "    def add_observation(self, state, action, reward, next_state, is_done):\n",
    "        self.replays.add(state, action, reward, next_state, is_done)\n",
    "        \n",
    "    def train(self):\n",
    "        states, actions, rewards, states_next, dones = self.replays.sample(self.batch_size)\n",
    "        qvalues = self.model.predict_batch(states)\n",
    "        qvalues_next = self.model.predict_batch(states_next)\n",
    "        y = build_training_set(qvalues, qvalues_next, actions, rewards, dones)\n",
    "        self.model.train(states, y)\n",
    "    \n",
    "    \n",
    "network = QNetwork(observation_shape, n_actions)\n",
    "agent = DQNAgent(network)\n",
    "s1 = env.reset()\n",
    "a = agent.policy(s1)\n",
    "s2, r, d, _ = env.step(a)\n",
    "agent.add_observation(s1, a, r, s2, d)\n",
    "agent.train()\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
