{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve CartPole using DQN\n",
    "\n",
    "https://gym.openai.com/envs/CartPole-v0/\n",
    "\n",
    "Some useful resources:\n",
    "  * https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from keras import models, layers, optimizers\n",
    "from replay_buffer import ReplayBuffer\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-04 09:13:34,655] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape: (4,)\n",
      "Number of actions: 2\n",
      "Example state: [ 0.03197971 -0.03378763  0.03882737  0.0253279 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe8dafa93c8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf/jLlghOLjuwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe8ed605160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#spawn game instance for tests\n",
    "env = gym.make(\"CartPole-v0\") #create raw env\n",
    "\n",
    "observation_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"Observation shape: {}\".format(observation_shape))\n",
    "print(\"Number of actions: {}\".format(n_actions))\n",
    "\n",
    "print(\"Example state: {}\".format(env.reset()))\n",
    "plt.imshow(env.render('rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define QNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00021796,  0.00107665], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class QNetwork:\n",
    "    \n",
    "    def __init__(self, input_shape, n_actions, alpha=0.0003):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.model = self._build_network()\n",
    "\n",
    "    def _build_network(self):\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.InputLayer(self.input_shape))\n",
    "        model.add(layers.Dense(64, activation='relu'))\n",
    "        model.add(layers.Dense(self.output_shape, activation='linear'))\n",
    "        opt = optimizers.RMSprop(lr=self.alpha)\n",
    "        model.compile(loss='mse', optimizer=opt)\n",
    "        return model\n",
    "    \n",
    "    def predict(self, state):\n",
    "        \"\"\"Make prediction for single state and return q values for all actions\"\"\"\n",
    "        s = np.expand_dims(state, axis=0)\n",
    "        return self.model.predict(s)[0]\n",
    "    \n",
    "    def predict_batch(self, states):\n",
    "        \"\"\"Make prediction for list of states\"\"\"\n",
    "        return self.model.predict(states)\n",
    "    \n",
    "    def train(self, x, y):\n",
    "        self.model.fit(x, y, batch_size=64, verbose=0)\n",
    "\n",
    "        \n",
    "network = QNetwork(observation_shape, n_actions)\n",
    "network.predict(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build traing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "def build_training_set(qvalues, qvalues_next, actions, rewards, dones, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Create training set for QNetwork.\n",
    "    Params:\n",
    "      qvalues           - Q values for the starting state\n",
    "      qvalues_next      - Q values for the state the next state\n",
    "      actions           - Actions taken\n",
    "      rewards           - Rewards received after taking action \n",
    "      dones             - Did this action end the episode?\n",
    "      \n",
    "    Returns:\n",
    "      Expected qvalues\n",
    "    \"\"\"\n",
    "    y = qvalues.copy()\n",
    "    next_rewards = np.where(dones, np.zeros(rewards.shape), np.max(qvalues_next, axis=1))\n",
    "    y[np.arange(y.shape[0]), actions] = rewards + gamma * next_rewards\n",
    "    return y\n",
    "\n",
    "\n",
    "# Some tests\n",
    "qvalues = np.zeros((5, n_actions))\n",
    "qvalues2 = np.ones((5, n_actions))\n",
    "actions = np.array([0, 1, 0, 1, 0])\n",
    "rewards = np.array([1, 2, 3, 4, 5])\n",
    "dones = np.array([False, False, False, False, True])\n",
    "expected_y = np.array([[2, 0], [0, 3], [4, 0], [0, 5], [5, 0]])\n",
    "y = build_training_set(qvalues, qvalues2, actions, rewards, dones, 1.0)\n",
    "assert np.array_equal(y, expected_y), 'Wrong expected qvalue calculated'\n",
    "print('Ok!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, network):\n",
    "        self.memory_capacity = 100000\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_max = 0.01\n",
    "        self.epsilon_lambda = 0.001\n",
    "        self.batch_size = 64\n",
    "        self.epsilon = self.epsilon_max        \n",
    "        self.model = network\n",
    "        self.replays = ReplayBuffer(self.memory_capacity)\n",
    "        self.step = 0\n",
    "        \n",
    "    def policy(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.randint(0, network.output_shape)\n",
    "        else:\n",
    "            qvalues = self.model.predict(state)\n",
    "            action = np.argmax(qvalues)\n",
    "            self.step += 1\n",
    "        self.epsilon = self.epsilon_min + (self.epsilon_max-self.epsilon_min) * np.power(np.e, self.epsilon_lambda*self.step)\n",
    "        return action\n",
    "    \n",
    "    def add_observation(self, state, action, reward, next_state, is_done):\n",
    "        self.replays.add(state, action, reward, next_state, is_done)\n",
    "        \n",
    "    def train(self):\n",
    "        states, actions, rewards, states_next, dones = self.replays.sample(self.batch_size)\n",
    "        qvalues = self.model.predict_batch(states)\n",
    "        qvalues_next = self.model.predict_batch(states_next)\n",
    "        y = build_training_set(qvalues, qvalues_next, actions, rewards, dones)\n",
    "        self.model.train(states, y)\n",
    "    \n",
    "    \n",
    "network = QNetwork(observation_shape, n_actions)\n",
    "agent = DQNAgent(network)\n",
    "s1 = env.reset()\n",
    "a = agent.policy(s1)\n",
    "s2, r, d, _ = env.step(a)\n",
    "agent.add_observation(s1, a, r, s2, d)\n",
    "agent.train()\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_session(env, agent, t_max=1000):\n",
    "    \n",
    "    states,actions = [],[]\n",
    "    total_reward = 0\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        # a vector of action probabilities in current state\n",
    "        action = agent.policy(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        #record sessions like you did before\n",
    "        agent.add_observation(state, action, reward, next_state, done)\n",
    "        agent.train()\n",
    "        total_reward += reward\n",
    "        \n",
    "        state = next_state\n",
    "        if done: break\n",
    "            \n",
    "    return total_reward\n",
    "\n",
    "\n",
    "agent = DQNAgent(QNetwork(observation_shape, n_actions))\n",
    "generate_session(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100, reward: 8.0\n",
      "Step: 200, reward: 10.0\n",
      "Step: 300, reward: 10.0\n",
      "Step: 400, reward: 8.0\n",
      "Step: 500, reward: 9.0\n",
      "Step: 600, reward: 10.0\n",
      "Step: 700, reward: 19.0\n",
      "Step: 800, reward: 10.0\n",
      "Step: 900, reward: 19.0\n",
      "Step: 1000, reward: 200.0\n",
      "Step: 1100, reward: 78.0\n",
      "Step: 1200, reward: 185.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-30cbf978be2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_sessions\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msession_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-a746dacbe131>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(env, agent, t_max)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#record sessions like you did before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-e9d2b4992361>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mqvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mqvalues_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_training_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqvalues_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-d36532fc7d7d>\u001b[0m in \u001b[0;36mpredict_batch\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;34m\"\"\"Make prediction for list of states\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/klangner/bin/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/klangner/bin/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1790\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/home/klangner/bin/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/klangner/bin/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/klangner/bin/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/klangner/bin/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/klangner/bin/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/klangner/bin/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/klangner/bin/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/klangner/bin/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_sessions = 2000\n",
    "\n",
    "agent = DQNAgent(QNetwork(observation_shape, n_actions))\n",
    "\n",
    "#generate new sessions\n",
    "rewards = []\n",
    "for i in range(1, n_sessions+1):\n",
    "    session_reward = generate_session(env, agent)\n",
    "    rewards.append(session_reward)\n",
    "    if i % 100 == 0:\n",
    "        mean_score = np.mean(rewards[-100:])\n",
    "        print('Step: {}, reward: {}'.format(i, mean_score))\n",
    "        if mean_score > 190:\n",
    "            print(\"You Won! in {} steps\".format(i))\n",
    "            break\n",
    "    \n",
    "plt.plot(rewards)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
